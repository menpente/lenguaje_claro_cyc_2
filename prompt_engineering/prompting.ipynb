{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frluquba/lenguaje_claro_cyc_2/blob/main/prompt_engineering/prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar la librería Py2neo.\n",
        "!pip install py2neo\n",
        "\n",
        "from py2neo import Graph\n",
        "\n",
        "# Conectar con la base de datos Neo4j mediante las credenciales del \"sandbox\".\n",
        "graph = Graph(\"bolt://98.84.45.155:7687\", auth=(\"neo4j\", \"requirement-photograph-correlations\"))\n",
        "\n",
        "# Con la cláusula \"MATCH\", se relacionan las entidades de cada nodo, siendo en este caso los actores con las películas donde actuaron. Se anota en un diccionario el objeto sobre el que queremos consultar (en este caso,las películas o \"{{title: '{movie_name}'}}\").\n",
        "movie_name = input(\"¿Sobre qué película necesitas saber los actores? \") # Prompt.\n",
        "query = f\"\"\"\n",
        "MATCH (a:Actor)-[:acted_in]->(m:Movie {{title: '{movie_name}'}})\n",
        "RETURN a, m\n",
        "\"\"\"\n",
        "\n",
        "# Se ejecuta la consulta.\n",
        "results = graph.run(query).data()\n",
        "\n",
        "if results:\n",
        "    for result in results:\n",
        "        actor = result['a']\n",
        "        movie = result['m']\n",
        "        prompt = f\"¿Quién actuó en la película {movie['title']}? La respuesta es {actor['name']}.\" # Pregunta y respuesta del prompt. Se anota, dentro de un diccionario y una lista, aquello sobre lo que se pregunta (\"{movie['title']}\"). Luego, se hace lo mismo con el objeto que va a ser la respuesta (\"{actor['name']}\").\n",
        "        print(prompt)\n",
        "else:\n",
        "    print(f\"No se encontraron actores para la película '{movie_name}'.\")"
      ],
      "metadata": {
        "id": "DSLqbFjYlk8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar la librería Py2neo.\n",
        "!pip install py2neo\n",
        "\n",
        "from py2neo import Graph\n",
        "\n",
        "# Conectar con la base de datos Neo4j mediante las credenciales del \"sandbox\".\n",
        "graph = Graph(\"bolt://98.84.45.155:7687\", auth=(\"neo4j\", \"requirement-photograph-correlations\"))\n",
        "\n",
        "\n",
        "# Con la cláusula \"MATCH\", se relacionan las entidades de cada nodo, siendo en este caso los actores con las películas donde actuaron. Se anota en un diccionario el objeto sobre el que queremos consultar (en este caso, los actores o \"{{name: '{actor_name}'}}'}\").\n",
        "actor_name = input(\"¿Sobre qué actor deseas preguntar? \") # Prompt.\n",
        "query = f\"\"\"\n",
        "MATCH (a:Actor {{name: '{actor_name}'}})-[:acted_in]->(m:Movie)\n",
        "RETURN a, m\n",
        "\"\"\"\n",
        "\n",
        "# Se ejecuta la consulta.\n",
        "results = graph.run(query).data()\n",
        "\n",
        "# Generar prompts automáticamente\n",
        "if results:\n",
        "    for result in results:\n",
        "        actor = result['a']\n",
        "        movie = result['m']\n",
        "        prompt = f\"¿En qué película actuó {actor['name']}? La respuesta es {movie['title']}.\" # Pregunta y respuesta del prompt. Se anota, dentro de un diccionario y una lista, aquello sobre lo que se pregunta (\"{actor['name']}\"). Luego, se hace lo mismo con el objeto que va a ser la respuesta (\"{movie['title']}\").\n",
        "        print(prompt)\n",
        "else:\n",
        "    print(f\"No se encontraron películas para el actor {actor_name}.\")"
      ],
      "metadata": {
        "id": "LxJLIJT4lk07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81oZ-MN-357S"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Registrarse en Hugging Face y obtener un token aquí: https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "56LTYo_b5p5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "Rsw4v5k5Crtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = pipeline('text-generation', model='microsoft/phi-2')\n",
        "# se puede cambiar el modelo por otro de los disponibles aquí https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407\n",
        "# algunos modelos requieren aceptar condiciones o registrarse\n",
        "# aquí mopodemos ver modelos específicos para español"
      ],
      "metadata": {
        "id": "9JPuw7Xk_615"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your prompts\n",
        "prompts = [\n",
        "    \"Explain the significance of prompt engineering in AI.\",\n",
        "    \"What are the benefits of using LLMs in education?\",\n",
        "    \"Describe the impact of AI on healthcare.\"\n",
        "]\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results = []\n",
        "\n",
        "# Generate responses for each prompt\n",
        "for prompt in prompts:\n",
        "    response = model(prompt, max_length=100)[0]['generated_text']\n",
        "    results.append({'prompt': prompt, 'response': response})\n",
        "\n",
        "# Create a DataFrame from the results\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('prompt_responses.csv', index=False)\n",
        "\n",
        "print(\"Results saved to prompt_responses.csv\")\n"
      ],
      "metadata": {
        "id": "pbmIvX_OCe5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PENDIENTE: explorar usar la función de memoria de langchain para guardar los resultados. https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/"
      ],
      "metadata": {
        "id": "feJvNRORYCwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}